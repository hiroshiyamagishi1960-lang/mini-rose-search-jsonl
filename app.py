# app.py â€” Mini Rose Search APIï¼ˆæ­£å¼ç°¡ç•¥ç‰ˆãƒ»JSONLå°‚ç”¨ï¼‰
# ç‰ˆ: stable-jsonl-2025-10-16-proper
# ç‰¹å¾´: æ—¥æœ¬èªæ¤œç´¢æœ€é©åŒ– / UIç¶­æŒ / è¨ºæ–­æ©Ÿèƒ½å®Œå‚™ / è»½é‡é«˜é€Ÿç‰ˆ

import os, re, json, unicodedata, datetime as dt, hashlib
from typing import List, Dict, Any
from fastapi import FastAPI, Query
from fastapi.responses import JSONResponse, HTMLResponse, PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import requests

# ==== åŸºæœ¬è¨­å®š ====
APP_VERSION = "stable-jsonl-2025-10-16-proper"
KB_URL = os.getenv("KB_URL", "").strip()
KB_PATH = "kb.jsonl"
CACHE_FILE = "kb_cache.jsonl"

# ==== FastAPI åˆæœŸåŒ– ====
app = FastAPI(title="Mini Rose Search API", version=APP_VERSION)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
app.mount("/static", StaticFiles(directory="static"), name="static")

# ==== æ­£è¦åŒ–é–¢æ•° ====
def normalize(text: str) -> str:
    """æ—¥æœ¬èªãƒ»è‹±æ•°å­—ã‚’çµ±ä¸€çš„ã«æ­£è¦åŒ–ã™ã‚‹"""
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = text.lower()
    # å¥èª­ç‚¹ãƒ»ç©ºç™½ã‚’æ•´ç†
    text = re.sub(r"[ã€ã€‚,ï¼ï¼Œï½¡ï½¥ãƒ»ã€Œã€ã€ã€ï¼ˆï¼‰()ï¼»ï¼½\[\]{}<>ã€ˆã€‰ã€ã€‘!?ï¼ï¼Ÿâ€¦â€¥]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def tokenize_like_japanese(text: str) -> List[str]:
    """
    æ—¥æœ¬èªç”¨ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆåˆ†ã‹ã¡æ›¸ãé¢¨ï¼‰
    é•·ã„å˜èªåˆ—ã‚’3ã€œ5æ–‡å­—å˜ä½ã§åˆ†å‰²ã—ã¦éƒ¨åˆ†ä¸€è‡´ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹
    """
    text = normalize(text)
    if not text:
        return []
    # è‹±æ•°å­—ã¨æ¼¢å­—ãƒ»ã‹ãªã‚’åˆ†é›¢
    parts = re.findall(r"[a-zA-Z0-9]+|[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]+", text)
    tokens = []
    for p in parts:
        if len(p) > 5:
            # é•·ã™ãã‚‹å˜èªã‚’ã‚¹ãƒ©ã‚¤ã‚¹
            tokens += [p[i:i+3] for i in range(0, len(p), 3)]
        else:
            tokens.append(p)
    return tokens

# ==== KB èª­ã¿è¾¼ã¿ ====
def load_kb() -> List[Dict[str, Any]]:
    """KBã‚’URLã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    if KB_URL:
        try:
            res = requests.get(KB_URL, timeout=10)
            if res.status_code == 200:
                with open(KB_PATH, "w", encoding="utf-8") as f:
                    f.write(res.text)
        except Exception as e:
            print("âš ï¸ KB_URL load failed:", e)
    path = KB_PATH if os.path.exists(KB_PATH) else CACHE_FILE
    if not os.path.exists(path):
        print("âš ï¸ No KB found.")
        return []
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                items.append(json.loads(line))
            except Exception:
                continue
    print(f"ğŸ“š KB loaded: {len(items)} records")
    return items

KB_DATA = load_kb()
KB_HASH = hashlib.sha256(json.dumps(KB_DATA, ensure_ascii=False).encode()).hexdigest()[:12]

# ==== ãƒã‚¤ãƒ©ã‚¤ãƒˆ ====
def highlight_text(text: str, query: str) -> str:
    """æ¤œç´¢èªå¥ã‚’ <mark> ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ"""
    q = re.escape(normalize(query))
    if not q:
        return text
    return re.sub(q, lambda m: f"<mark>{m.group(0)}</mark>", text, flags=re.IGNORECASE)

# ==== æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ====
def search_kb(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    q_norm = normalize(query)
    tokens = tokenize_like_japanese(q_norm)
    if not tokens:
        return []

    results = []
    for rec in KB_DATA:
        title = rec.get("title", "")
        content = rec.get("content", "")
        url = rec.get("url", "")
        full_text = normalize(title + " " + content)
        score = sum(1 for t in tokens if t in full_text)
        if score > 0:
            snippet = content[:1200]
            snippet = highlight_text(snippet, q_norm)
            results.append({
                "title": title or "(ç„¡é¡Œ)",
                "content": snippet,
                "url": url,
                "source": url,
                "score": score
            })

    results.sort(key=lambda x: x["score"], reverse=True)
    return results[:top_k]

# ==== API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ====
@app.get("/health2")
def health2():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹"""
    return {
        "ok": True,
        "has_kb": bool(KB_DATA),
        "kb_size": len(KB_DATA),
        "kb_url": KB_URL,
        "kb_hash": KB_HASH
    }

@app.get("/diag2")
def diag2():
    """è©³ç´°è¨ºæ–­"""
    return {
        "version_hint": "jsonl-diag2",
        "kb_url": KB_URL,
        "has_kb": bool(KB_DATA),
        "kb_size": len(KB_DATA),
        "kb_sha": KB_HASH,
        "loaded_at": dt.datetime.now().isoformat(timespec="seconds")
    }

@app.get("/api/search")
def api_search(
    q: str = Query(..., description="æ¤œç´¢èªå¥"),
    page: int = 1,
    page_size: int = 5,
):
    """KBå…¨æ–‡æ¤œç´¢"""
    if not q.strip():
        return {"items": [], "total_hits": 0, "error": "empty query"}
    results = search_kb(q, top_k=page_size)
    total = len(results)
    return {
        "items": results,
        "total_hits": total,
        "page": page,
        "page_size": page_size,
        "has_more": total > page * page_size,
        "next_page": page + 1 if total > page * page_size else None
    }

@app.get("/ui")
def ui_page():
    """æ¤œç´¢UI"""
    try:
        with open("static/ui.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except FileNotFoundError:
        return HTMLResponse("<h2>UI not found</h2>", status_code=404)

@app.get("/version")
def version():
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±"""
    return PlainTextResponse(APP_VERSION)

@app.get("/")
def root():
    """ãƒ«ãƒ¼ãƒˆç¢ºèª"""
    return {"message": "Mini Rose Search API running", "version": APP_VERSION}

# ==== ã“ã“ã¾ã§ ====
